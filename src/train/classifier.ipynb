{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee118f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sdb1/home/kygrachev/diploma/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "model_name = \"google-bert/bert-base-uncased\" #\"SamLowe/roberta-base-go_emotions\"\n",
    "\n",
    "\n",
    "from datasets import load_from_disk, Dataset\n",
    "\n",
    "data = load_from_disk(\"../../data/google-research-datasets-go_emotions\")\n",
    "\n",
    "\n",
    "\n",
    "from transformers import DataCollatorWithPadding, AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", max_length=512, truncation=True)\n",
    "\n",
    "\n",
    "def make_loader(dataset, batch_size: int = 16):\n",
    "    dataset = dataset.map(tokenize_function, batched=True)\n",
    "    dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "\n",
    "unpack_label = {\n",
    "    0: \"admiration\",\n",
    "    1: \"anger\",\n",
    "    2: \"annoyance\",\n",
    "    3: \"disappointment\",\n",
    "    4: \"disapproval\",\n",
    "    5: \"disgust\",\n",
    "    6: \"excitement\",\n",
    "    7: \"gratitude\",\n",
    "    8: \"joy\",\n",
    "    9: \"optimism\",\n",
    "    10: \"sadness\",\n",
    "    11: \"neutral\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5652e434",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = make_loader(data[\"train\"])\n",
    "eval_data = make_loader(data[\"validation\"])\n",
    "test_data = make_loader(data[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "993c125c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0,  ..., 0, 0, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 1],\n",
      "        ...,\n",
      "        [1, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 1, 0,  ..., 0, 0, 0],\n",
      "        [0, 1, 0,  ..., 0, 0, 0]], dtype=torch.int8)\n",
      "torch.Size([23597, 12])\n",
      "tensor([4116, 1564, 2628, 1266, 2018,  791,  843, 2618, 1689, 1575, 1375, 6085])\n",
      "tensor([ 4.7330, 14.0876,  7.9791, 17.6390, 10.6933, 28.8319, 26.9917,  8.0134,\n",
      "        12.9710, 13.9822, 16.1615,  2.8779])\n",
      "tensor([1.3036, 1.5739, 1.3974, 1.6766, 1.4758, 2.0000, 1.9468, 1.3984, 1.5417,\n",
      "        1.5709, 1.6339, 1.2500])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "labels = torch.tensor(data[\"train\"][\"labels\"], dtype=torch.int8)\n",
    "print(labels)\n",
    "print(labels.shape)\n",
    "\n",
    "classes = labels.sum(dim=0)\n",
    "print(classes)\n",
    "\n",
    "pos_weight = len(labels) / classes - 1\n",
    "print(pos_weight)\n",
    "\n",
    "mn, mx = pos_weight.min(), pos_weight.max()\n",
    "scale_mn, scale_mx = 1.25, 2\n",
    "pos_weight = (pos_weight - mn) / (mx - mn) * (scale_mx - scale_mn) + scale_mn\n",
    "print(pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3bf345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def count_matches(preds, labels, matches):\n",
    "    for pred, gt in zip(preds, labels):\n",
    "        pred, gt = {i for i, j in enumerate(pred) if j}, {i for i, j in enumerate(gt) if j}\n",
    "        for lab in gt & pred:\n",
    "            matches[lab][\"tp\"] += 1\n",
    "        for lab in pred - gt:\n",
    "            matches[lab][\"fp\"] += 1\n",
    "        for lab in gt - pred:\n",
    "            matches[lab][\"fn\"] += 1\n",
    "\n",
    "\n",
    "def calc_metrics(tp=0, fp=0, fn=0, tn=0):\n",
    "    accuracy = (tp + tn) / (tp + fp + tn + fn) if (tp + fp + tn + fn) else 0.0\n",
    "    precision = tp / (fp + tp) if (fp + tp) else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "    f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) else 0.0\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1-score\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b581bd",
   "metadata": {},
   "source": [
    "## Оптимизация с дополнительным смещением логитов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21c69850",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at SamLowe/roberta-base-go_emotions and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([28]) in the checkpoint and torch.Size([12]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([28, 768]) in the checkpoint and torch.Size([12, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "\n",
    "def make_forward(model):\n",
    "    forward_func = model.forward\n",
    "    loss_func = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    def forward(**inputs):\n",
    "        labels = inputs.pop(\"labels\") if \"labels\" in inputs else None\n",
    "        output = forward_func(**inputs)\n",
    "        logits = output.logits - model.classifier.custom_levels\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            labels = labels.to(logits.device)\n",
    "            loss = loss_func(logits, labels.float())\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=output.hidden_states,\n",
    "            attentions=output.attentions,\n",
    "        )\n",
    "\n",
    "    return forward\n",
    "\n",
    "\n",
    "def make_predict(model):\n",
    "    def predict(**inputs):\n",
    "        logits = model(**inputs).logits\n",
    "        probs = torch.sigmoid(logits)\n",
    "        return [torch.where(p > 0.5)[0].tolist() for p in probs]\n",
    "\n",
    "    return predict\n",
    "\n",
    "\n",
    "def init_model(model_name, layers_to_finetune: int = 5):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(unpack_label),\n",
    "        ignore_mismatched_sizes=True,\n",
    "    )\n",
    "    model.classifier.custom_levels = torch.nn.Parameter(\n",
    "        torch.randn(\n",
    "            size=(1, model.num_labels),\n",
    "            requires_grad=True,\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "    )\n",
    "    model.forward = make_forward(model)\n",
    "    model.predict = make_predict(model)\n",
    "\n",
    "    # 1. Замораживаем все слои модели\n",
    "    for param in model.base_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # 2. Размораживаем последние layers_to_finetune слоёв\n",
    "    for param in model.base_model.encoder.layer[-layers_to_finetune:].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # 3. Убеждаемся, что классификатор тоже обучается (если нужно)\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    return model.to(device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = init_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c0480b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"../../models/classifier/optimize_custom_levels/model_best/model.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7788316f",
   "metadata": {},
   "source": [
    "## Оптимизация с нормализацией логитов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac5c23cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at SamLowe/roberta-base-go_emotions and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([28]) in the checkpoint and torch.Size([12]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([28, 768]) in the checkpoint and torch.Size([12, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import BatchNorm1d\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def make_forward(model):\n",
    "    forward_func = model.forward\n",
    "    loss_func = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n",
    "\n",
    "    def forward(**inputs):\n",
    "        labels = inputs.pop(\"labels\") if \"labels\" in inputs else None\n",
    "        output = forward_func(**inputs)\n",
    "        bn_logits = model.classifier.normalizer(output.logits)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            labels = labels.to(bn_logits.device)\n",
    "            loss = loss_func(bn_logits, labels.float())\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=bn_logits,\n",
    "            hidden_states=output.hidden_states,\n",
    "            attentions=output.attentions,\n",
    "        )\n",
    "\n",
    "    return forward\n",
    "\n",
    "\n",
    "def make_predict(model):\n",
    "    def predict(**inputs):\n",
    "        logits = model(**inputs).logits\n",
    "        probs = torch.sigmoid(logits)\n",
    "        return [torch.where(p > 0.5)[0].tolist() for p in probs]\n",
    "\n",
    "    return predict\n",
    "\n",
    "\n",
    "def init_model(model_name, layers_to_finetune: int = 5):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(unpack_label),\n",
    "        ignore_mismatched_sizes=True,\n",
    "    )\n",
    "    model.classifier.normalizer = BatchNorm1d(model.num_labels)\n",
    "    model.forward = make_forward(model)\n",
    "    model.predict = make_predict(model)\n",
    "\n",
    "    # 1. Замораживаем все слои модели\n",
    "    for param in model.base_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # 2. Размораживаем последние layers_to_finetune слоёв\n",
    "    for param in model.base_model.encoder.layer[-layers_to_finetune:].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # 3. Убеждаемся, что классификатор тоже обучается (если нужно)\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    return model.to(device)\n",
    "\n",
    "model = init_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782944b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load(\"../../models/classifier/bn/model_best/model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eda6f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "f1_macro_best = threshold_best = 0.5\n",
    "epoches = 10\n",
    "batch_size = 64\n",
    "optimizer = AdamW(model.parameters(), lr=1.5e-5)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.65, patience=0, verbose=True)\n",
    "\n",
    "export_path = \"./models/classifier/bn/\"\n",
    "os.makedirs(export_path, exist_ok=True)\n",
    "os.makedirs(export_path + \"model_best\", exist_ok=True)\n",
    "os.makedirs(export_path + \"model_last\", exist_ok=True)\n",
    "\n",
    "thresholds = [round(v, 2) for v in torch.arange(0.5, 0.75, 0.02).tolist()]\n",
    "\n",
    "metrics = {\"train\": {}, \"eval\": {}}\n",
    "for epoch in range(epoches):\n",
    "    print(f\"\\n{epoch=}\")\n",
    "    \n",
    "    model.train()\n",
    "    train_losses, train_metrics = [], defaultdict(lambda: defaultdict(int))\n",
    "    for batch in tqdm(train_data):\n",
    "        batch.to(model.device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        outputs.loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(outputs.loss.item())\n",
    "\n",
    "        count_matches(\n",
    "            (torch.sigmoid(outputs.logits) > threshold_best).int().tolist(),\n",
    "            batch[\"labels\"].tolist(),\n",
    "            train_metrics,\n",
    "        )\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    metrics[\"train\"][epoch] = {unpack_label[lab]: calc_metrics(**train_metrics[lab]) for lab in unpack_label}\n",
    "    f1_macro = sum(m[\"f1-score\"] for m in metrics[\"train\"][epoch].values()) / len(unpack_label)\n",
    "    metrics[\"train\"][epoch][\"f1_macro\"] = f1_macro\n",
    "\n",
    "    print(f\"train loss={sum(train_losses) / len(train_losses):.4f}; f1_macro={f1_macro:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    eval_losses, eval_metrics = [], defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(eval_data):\n",
    "            batch.to(model.device)\n",
    "            outputs = model(**batch)\n",
    "\n",
    "            eval_losses.append(outputs.loss.item())\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                count_matches(\n",
    "                    (torch.sigmoid(outputs.logits) > threshold).int().tolist(),\n",
    "                    batch[\"labels\"].tolist(),\n",
    "                    eval_metrics[threshold],\n",
    "                )\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "    m_vals = {}\n",
    "    for threshold in thresholds:\n",
    "        m_vals[threshold] = {\n",
    "            unpack_label[lab]: calc_metrics(**eval_metrics[threshold][lab]) for lab in unpack_label\n",
    "        }\n",
    "\n",
    "        f1_macro = sum(m[\"f1-score\"] for m in m_vals[threshold].values()) / len(unpack_label)\n",
    "\n",
    "        m_vals[threshold][\"f1_macro\"] = f1_macro\n",
    "    \n",
    "        if f1_macro > f1_macro_best:\n",
    "            f1_macro_best = f1_macro\n",
    "            threshold_best = threshold\n",
    "            torch.save(model.state_dict(), os.path.join(export_path, \"model_best\", \"model.pt\"))\n",
    "    \n",
    "        print(f\"{threshold=}; {f1_macro=:.4f}\")\n",
    "    \n",
    "    metrics[\"eval\"][epoch] = m_vals\n",
    "\n",
    "    val_loss = sum(eval_losses) / len(eval_losses)\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    print(f\"{val_loss=:.4f}; {f1_macro_best=:.4f}\")\n",
    "    \n",
    "torch.save(model.state_dict(), os.path.join(export_path, \"model_last\", \"model.pt\"))\n",
    "\n",
    "with open(os.path.join(export_path, \"metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368c9a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 248/248 [03:53<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_macro=0.4666; f1_macro_best=0.0000\n",
      "f1_macro_best=0.46661603613552205; threshold_best=0.5\n",
      "f1_macro=0.5227; f1_macro_best=0.4666\n",
      "f1_macro_best=0.5226749234909143; threshold_best=0.6\n",
      "f1_macro=0.5351; f1_macro_best=0.5227\n",
      "f1_macro_best=0.5350511492792154; threshold_best=0.65\n",
      "f1_macro=0.5418; f1_macro_best=0.5351\n",
      "f1_macro_best=0.5417763695171706; threshold_best=0.7\n",
      "f1_macro=0.5483; f1_macro_best=0.5418\n",
      "f1_macro_best=0.5482732668662416; threshold_best=0.75\n",
      "f1_macro=0.5417; f1_macro_best=0.5483\n",
      "f1_macro_best=0.5482732668662416; threshold_best=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "metrics = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "\n",
    "f1_macro_best = 0\n",
    "threshold_best = 0.5\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(eval_data):\n",
    "        batch.to(model.device)\n",
    "        outputs = model(**batch)\n",
    "        \n",
    "        for threshold in [0.5, 0.6, 0.65, 0.7, 0.75, 0.8]:\n",
    "            count_matches(\n",
    "                (torch.sigmoid(outputs.logits) > threshold).int().tolist(),\n",
    "                batch[\"labels\"].tolist(),\n",
    "                metrics[threshold],\n",
    "            )\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    for threshold in [0.5, 0.6, 0.65, 0.7, 0.75, 0.8]:\n",
    "        metrics[threshold] = {unpack_label[lab]: calc_metrics(**metrics[threshold][lab]) for lab in unpack_label}\n",
    "\n",
    "        f1_macro = sum(m[\"f1-score\"] for m in metrics[threshold].values()) / len(unpack_label)\n",
    "\n",
    "        metrics[threshold][\"f1_macro\"] = f1_macro\n",
    "\n",
    "        print(f\"{f1_macro=:.4f}; {f1_macro_best=:.4f}\")\n",
    "\n",
    "        if f1_macro > f1_macro_best:\n",
    "            f1_macro_best = f1_macro\n",
    "            threshold_best = threshold\n",
    "        \n",
    "        print(f\"{f1_macro_best=}; {threshold_best=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2c06ac",
   "metadata": {},
   "source": [
    "## Оптимизация уровней активации для каждого класса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9fdce79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "model_name = \"google-bert/bert-base-uncased\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class MultilabelFocalLoss:\n",
    "\n",
    "    def __init__(self, pos_weight=None):\n",
    "        self.pos_weight = pos_weight\n",
    "\n",
    "\n",
    "    def __call__(self, logits, targets, alpha=0.25, gamma=2.0, eps=1e-5):\n",
    "        targets = targets.float()\n",
    "\n",
    "        if self.pos_weight is not None:\n",
    "            alphas = ((targets * (self.pos_weight - 1)) + alpha * (1 - targets)) \n",
    "        else:\n",
    "            alphas = alpha\n",
    "      \n",
    "        probs = torch.sigmoid(logits)\n",
    "        pt = probs * targets + (1 - probs) * (1 - targets)\n",
    "        focal_loss = -alphas * ((1 - pt) ** gamma * torch.log(pt + eps))\n",
    "\n",
    "        return focal_loss.sum()\n",
    "\n",
    "\n",
    "class BertForMultiLabelClassification(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            num_labels: int, \n",
    "            pos_weight: torch.tensor, \n",
    "            device: torch.device\n",
    "        ):\n",
    "        super(BertForMultiLabelClassification, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels, device=device)\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n",
    "        self.threshold_loss_fn = MultilabelFocalLoss(pos_weight=pos_weight.to(device))\n",
    "        self.threshold_levels = nn.Parameter(torch.zeros(num_labels, device=device), requires_grad=True)\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def forward(self, **inputs) -> SequenceClassifierOutput:\n",
    "        labels = inputs.pop(\"labels\") if \"labels\" in inputs else None\n",
    "\n",
    "        outputs = self.bert(**inputs)\n",
    "        pooled_output = outputs.pooler_output  # [CLS] токен\n",
    "        x = self.dropout(pooled_output)\n",
    "        logits = self.classifier(x)  # логиты\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            labels = labels.to(self.device)\n",
    "            loss = self.loss_fn(logits, labels)  # labels.float())\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def thresholds(self):\n",
    "        return torch.sigmoid(self.threshold_levels)  # Ограничение порогов (0; 1)\n",
    "\n",
    "\n",
    "model = BertForMultiLabelClassification(\n",
    "    num_labels=len(unpack_label), \n",
    "    pos_weight=pos_weight, \n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5a6b78e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./models/classifier/optimize_levels_on_test/model_epoch/model1.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48edef08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1475 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1475/1475 [10:44<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss=0.1462; f1_macro=0.7300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 185/185 [00:22<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss=0.2069; f1_macro=0.6058; f1_macro_best=0.6058\n",
      "\n",
      "epoch=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1475/1475 [10:44<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss=0.1116; f1_macro=0.7995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 185/185 [00:22<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss=0.2167; f1_macro=0.6188; f1_macro_best=0.6188\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "f1_macro_best = 0.5\n",
    "epoches = 10\n",
    "batch_size = 64\n",
    "threshold_learn_speed = 5\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.7, patience=0, verbose=True)\n",
    "\n",
    "threshold_optimizer = AdamW([model.threshold_levels], lr=2e-4)\n",
    "threshold_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=0, verbose=True)\n",
    "\n",
    "export_path = \"./models/classifier/optimize_levels_on_test/\"\n",
    "os.makedirs(export_path, exist_ok=True)\n",
    "os.makedirs(export_path + \"model_best\", exist_ok=True)\n",
    "os.makedirs(export_path + \"model_last\", exist_ok=True)\n",
    "os.makedirs(export_path + \"model_epoch\", exist_ok=True)\n",
    "\n",
    "\n",
    "try:\n",
    "    metrics = {\"train\": {}, \"eval\": {}}\n",
    "    for epoch in range(2, epoches):\n",
    "        print(f\"\\n{epoch=}\")\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        # Оптимизация классификатора\n",
    "        train_losses, train_metrics = [], defaultdict(lambda: defaultdict(int))\n",
    "        for batch in tqdm(train_data):\n",
    "            batch.to(model.device)\n",
    "\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**batch)\n",
    "            outputs.loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(outputs.loss.item())\n",
    "\n",
    "            count_matches(\n",
    "                (torch.sigmoid(outputs.logits) > model.thresholds).int().tolist(),\n",
    "                batch[\"labels\"].tolist(),\n",
    "                train_metrics,\n",
    "            )\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        metrics[\"train\"][epoch] = {unpack_label[lab]: calc_metrics(**train_metrics[lab]) for lab in unpack_label}\n",
    "        f1_macro = sum(m[\"f1-score\"] for m in metrics[\"train\"][epoch].values()) / len(unpack_label)\n",
    "        metrics[\"train\"][epoch][\"f1_macro\"] = f1_macro\n",
    "\n",
    "        print(f\"train loss={sum(train_losses) / len(train_losses):.4f}; f1_macro={f1_macro:.4f}\")\n",
    "\n",
    "        if epoch < 2:\n",
    "            torch.save(model.state_dict(), os.path.join(export_path, \"model_epoch\", f\"model{epoch}.pt\"))\n",
    "        \n",
    "        # Валидация\n",
    "        model.eval()\n",
    "        eval_losses, eval_metrics = [], defaultdict(lambda: defaultdict(int))\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(eval_data):\n",
    "                batch.to(model.device)\n",
    "                outputs = model(**batch)\n",
    "\n",
    "                eval_losses.append(outputs.loss.item())\n",
    "\n",
    "                count_matches(\n",
    "                    (torch.sigmoid(outputs.logits) > model.thresholds).int().tolist(),\n",
    "                    batch[\"labels\"].tolist(),\n",
    "                    eval_metrics,\n",
    "                )\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "        metrics[\"eval\"][epoch] = {\n",
    "            unpack_label[lab]: calc_metrics(**eval_metrics[lab]) for lab in unpack_label\n",
    "        }\n",
    "\n",
    "        f1_macro = sum(m[\"f1-score\"] for m in metrics[\"eval\"][epoch].values()) / len(unpack_label)\n",
    "        metrics[\"eval\"][epoch][\"f1_macro\"] = f1_macro\n",
    "        if f1_macro > f1_macro_best:\n",
    "            f1_macro_best = f1_macro\n",
    "            torch.save(model.state_dict(), os.path.join(export_path, \"model_best\", \"model.pt\"))\n",
    "\n",
    "        val_loss = sum(eval_losses) / len(eval_losses)\n",
    "        scheduler.step(f1_macro)\n",
    "        \n",
    "        print(f\"{val_loss=:.4f}; {f1_macro=:.4f}; {f1_macro_best=:.4f}\")\n",
    "\n",
    "        if input() != \"\":\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    torch.save(model.state_dict(), os.path.join(export_path, \"model_last\", \"model.pt\"))\n",
    "\n",
    "    with open(os.path.join(export_path, \"metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51a3128f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1475/1475 [02:57<00:00,  8.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold loss=1.4135\n",
      "model.thresholds=tensor([0.5015, 0.4916, 0.4393, 0.4306, 0.4606, 0.4673, 0.4807, 0.4628, 0.4648,\n",
      "        0.4765, 0.4885, 0.4448], device='cuda:0', grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 185/185 [00:22<00:00,  8.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss=0.2170; f1_macro=0.6214; f1_macro_best=0.6214\n",
      "\n",
      "epoch=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1475/1475 [02:57<00:00,  8.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold loss=1.3756\n",
      "model.thresholds=tensor([0.5076, 0.4982, 0.4176, 0.4104, 0.4524, 0.4599, 0.4742, 0.4488, 0.4620,\n",
      "        0.4854, 0.4991, 0.4200], device='cuda:0', grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 185/185 [00:22<00:00,  8.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss=0.2170; f1_macro=0.6223; f1_macro_best=0.6223\n",
      "\n",
      "epoch=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1475/1475 [02:57<00:00,  8.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold loss=1.3493\n",
      "model.thresholds=tensor([0.5127, 0.5039, 0.4008, 0.3931, 0.4453, 0.4524, 0.4683, 0.4362, 0.4597,\n",
      "        0.4936, 0.5085, 0.3990], device='cuda:0', grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 185/185 [00:22<00:00,  8.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss=0.2168; f1_macro=0.6238; f1_macro_best=0.6238\n",
      "\n",
      "epoch=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1475/1475 [02:57<00:00,  8.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold loss=1.3318\n",
      "model.thresholds=tensor([0.5169, 0.5087, 0.3887, 0.3789, 0.4399, 0.4460, 0.4631, 0.4260, 0.4579,\n",
      "        0.5000, 0.5171, 0.3828], device='cuda:0', grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 185/185 [00:22<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss=0.2168; f1_macro=0.6243; f1_macro_best=0.6243\n",
      "\n",
      "epoch=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1475/1475 [02:57<00:00,  8.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold loss=1.3204\n",
      "model.thresholds=tensor([0.5205, 0.5126, 0.3795, 0.3673, 0.4348, 0.4404, 0.4587, 0.4179, 0.4562,\n",
      "        0.5058, 0.5244, 0.3698], device='cuda:0', grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 185/185 [00:22<00:00,  8.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss=0.2166; f1_macro=0.6246; f1_macro_best=0.6246\n",
      "\n",
      "epoch=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 38/1475 [00:04<02:56,  8.16it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m     threshold_loss.backward()\n\u001b[32m     20\u001b[39m     threshold_optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     threshold_losses.append(\u001b[43mthreshold_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     24\u001b[39m     torch.cuda.empty_cache()\n\u001b[32m     26\u001b[39m threshold_scheduler.step(\u001b[38;5;28msum\u001b[39m(threshold_losses) / \u001b[38;5;28mlen\u001b[39m(threshold_losses))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for epoch in range(epoches):        \n",
    "        print(f\"\\n{epoch=}\")\n",
    "        \n",
    "        # Оптимизация трешхолдов\n",
    "        threshold_losses = []\n",
    "        for batch in tqdm(train_data):\n",
    "            batch.to(model.device)\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "\n",
    "            model.train()\n",
    "            threshold_optimizer.zero_grad()\n",
    "            model_probs = torch.sigmoid(outputs.logits)\n",
    "            threshold_logits = (model_probs - model.thresholds) * threshold_learn_speed\n",
    "            threshold_loss = model.threshold_loss_fn(threshold_logits, batch[\"labels\"])\n",
    "            threshold_loss.backward()\n",
    "            threshold_optimizer.step()\n",
    "\n",
    "            threshold_losses.append(threshold_loss.item())\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        threshold_scheduler.step(sum(threshold_losses) / len(threshold_losses))\n",
    "        print(f\"threshold loss={sum(threshold_losses) / len(threshold_losses):.4f}\")\n",
    "        print(f\"{model.thresholds=}\")\n",
    "\n",
    "        # Валидация\n",
    "        model.eval()\n",
    "        eval_losses, eval_metrics = [], defaultdict(lambda: defaultdict(int))\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(eval_data):\n",
    "                batch.to(model.device)\n",
    "                outputs = model(**batch)\n",
    "\n",
    "                eval_losses.append(outputs.loss.item())\n",
    "\n",
    "                count_matches(\n",
    "                    (torch.sigmoid(outputs.logits) > model.thresholds).int().tolist(),\n",
    "                    batch[\"labels\"].tolist(),\n",
    "                    eval_metrics,\n",
    "                )\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "        metrics[\"eval\"][epoch] = {\n",
    "            unpack_label[lab]: calc_metrics(**eval_metrics[lab]) for lab in unpack_label\n",
    "        }\n",
    "\n",
    "        f1_macro = sum(m[\"f1-score\"] for m in metrics[\"eval\"][epoch].values()) / len(unpack_label)\n",
    "        metrics[\"eval\"][epoch][\"f1_macro\"] = f1_macro\n",
    "        if f1_macro > f1_macro_best:\n",
    "            f1_macro_best = f1_macro\n",
    "            torch.save(model.state_dict(), os.path.join(export_path, \"model_best\", \"model.pt\"))\n",
    "\n",
    "        val_loss = sum(eval_losses) / len(eval_losses)\n",
    "        scheduler.step(f1_macro)\n",
    "        \n",
    "        print(f\"{val_loss=:.4f}; {f1_macro=:.4f}; {f1_macro_best=:.4f}\")\n",
    "\n",
    "        if input() != \"\":\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    torch.save(model.state_dict(), os.path.join(export_path, \"model_last\", \"model.pt\"))\n",
    "\n",
    "    with open(os.path.join(export_path, \"metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da37f80e",
   "metadata": {},
   "source": [
    "## ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81a462a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "model_name = \"google-bert/bert-base-uncased\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class MultilabelFocalLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, pos_weight=None):\n",
    "        super().__init__()\n",
    "        self.pos_weight = pos_weight\n",
    "\n",
    "\n",
    "    def forward(self, logits, targets, alpha=0.5, gamma=2.0, eps=1e-5):\n",
    "        targets = targets.float()\n",
    "\n",
    "        if self.pos_weight is not None:\n",
    "            alphas = ((targets * (self.pos_weight - 1)) + alpha * (1 - targets)) \n",
    "        else:\n",
    "            alphas = alpha\n",
    "      \n",
    "        probs = torch.sigmoid(logits)\n",
    "        pt = probs * targets + (1 - probs) * (1 - targets)\n",
    "        focal_loss = -alphas * ((1 - pt) ** gamma * torch.log(pt + eps))\n",
    "\n",
    "        return focal_loss.mean()\n",
    "\n",
    "\n",
    "class BertForMultiLabelClassification(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            num_labels: int, \n",
    "            pos_weight: torch.Tensor, \n",
    "            device: torch.device\n",
    "        ):\n",
    "        super(BertForMultiLabelClassification, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels, device=device)\n",
    "        self.loss_fn = MultilabelFocalLoss(pos_weight=pos_weight.to(device))\n",
    "        self.threshold_levels = nn.Parameter(torch.zeros(num_labels, device=device), requires_grad=True)\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def forward(self, sharphess_coef: float = 5., **inputs) -> SequenceClassifierOutput:\n",
    "        labels = inputs.pop(\"labels\") if \"labels\" in inputs else None\n",
    "\n",
    "        outputs = self.bert(**inputs)\n",
    "        pooled_output = outputs.pooler_output  # [CLS] токен\n",
    "        x = self.dropout(pooled_output)\n",
    "        logits = self.classifier(x)  # логиты\n",
    "        cut_logits = (torch.sigmoid(logits) - self.thresholds) * sharphess_coef\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            labels = labels.to(self.device)\n",
    "            loss = self.loss_fn(cut_logits, labels)  # labels.float())\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=cut_logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def thresholds(self):\n",
    "        return torch.sigmoid(self.threshold_levels)  # Ограничение порогов (0; 1)\n",
    "\n",
    "\n",
    "model = BertForMultiLabelClassification(\n",
    "    num_labels=len(unpack_label), \n",
    "    pos_weight=pos_weight, \n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d52e4d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1475 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1475/1475 [10:42<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss=0.0254; f1_macro=0.4526\n",
      "model.thresholds=tensor([0.5009, 0.5007, 0.5005, 0.5005, 0.5006, 0.5007, 0.5007, 0.5012, 0.5008,\n",
      "        0.5005, 0.5006, 0.5004], device='cuda:0', grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 185/185 [00:22<00:00,  8.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss=0.0199; f1_macro=0.5559; f1_macro_best=0.5559\n",
      "\n",
      "epoch=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1475/1475 [10:42<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss=0.0184; f1_macro=0.6279\n",
      "model.thresholds=tensor([0.5017, 0.5013, 0.5007, 0.5009, 0.5010, 0.5012, 0.5012, 0.5022, 0.5014,\n",
      "        0.5010, 0.5014, 0.5006], device='cuda:0', grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 185/185 [00:22<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss=0.0200; f1_macro=0.5906; f1_macro_best=0.5906\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "f1_macro_best = 0.0\n",
    "epoches = 10\n",
    "batch_size = 64\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.2, patience=0, verbose=True)\n",
    "\n",
    "export_path = \"./models/classifier/optimize_thresholds/\"\n",
    "os.makedirs(export_path, exist_ok=True)\n",
    "os.makedirs(export_path + \"model_best\", exist_ok=True)\n",
    "os.makedirs(export_path + \"model_last\", exist_ok=True)\n",
    "os.makedirs(export_path + \"model_epoch\", exist_ok=True)\n",
    "\n",
    "\n",
    "try:\n",
    "    metrics = {\"train\": {}, \"eval\": {}}\n",
    "    for epoch in range(epoches):\n",
    "        print(f\"\\n{epoch=}\")\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        # Оптимизация классификатора\n",
    "        train_losses, train_metrics = [], defaultdict(lambda: defaultdict(int))\n",
    "        for batch in tqdm(train_data):\n",
    "            batch.to(model.device)\n",
    "\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**batch)\n",
    "            outputs.loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(outputs.loss.item())\n",
    "\n",
    "            count_matches(\n",
    "                (torch.sigmoid(outputs.logits) > 0.5).int().tolist(),\n",
    "                batch[\"labels\"].tolist(),\n",
    "                train_metrics,\n",
    "            )\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        metrics[\"train\"][epoch] = {unpack_label[lab]: calc_metrics(**train_metrics[lab]) for lab in unpack_label}\n",
    "        f1_macro = sum(m[\"f1-score\"] for m in metrics[\"train\"][epoch].values()) / len(unpack_label)\n",
    "        metrics[\"train\"][epoch][\"f1_macro\"] = f1_macro\n",
    "\n",
    "        print(f\"train loss={sum(train_losses) / len(train_losses):.4f}; f1_macro={f1_macro:.4f}\")\n",
    "        print(f\"{model.thresholds=}\")\n",
    "\n",
    "        if epoch < 2:\n",
    "            torch.save(model.state_dict(), os.path.join(export_path, \"model_epoch\", f\"model{epoch}.pt\"))\n",
    "        \n",
    "        # Валидация\n",
    "        model.eval()\n",
    "        eval_losses, eval_metrics = [], defaultdict(lambda: defaultdict(int))\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(eval_data):\n",
    "                batch.to(model.device)\n",
    "                outputs = model(**batch)\n",
    "\n",
    "                eval_losses.append(outputs.loss.item())\n",
    "\n",
    "                count_matches(\n",
    "                    (torch.sigmoid(outputs.logits) > 0.5).int().tolist(),\n",
    "                    batch[\"labels\"].tolist(),\n",
    "                    eval_metrics,\n",
    "                )\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "        metrics[\"eval\"][epoch] = {\n",
    "            unpack_label[lab]: calc_metrics(**eval_metrics[lab]) for lab in unpack_label\n",
    "        }\n",
    "\n",
    "        f1_macro = sum(m[\"f1-score\"] for m in metrics[\"eval\"][epoch].values()) / len(unpack_label)\n",
    "        metrics[\"eval\"][epoch][\"f1_macro\"] = f1_macro\n",
    "        if f1_macro > f1_macro_best:\n",
    "            f1_macro_best = f1_macro\n",
    "            torch.save(model.state_dict(), os.path.join(export_path, \"model_best\", \"model.pt\"))\n",
    "\n",
    "        val_loss = sum(eval_losses) / len(eval_losses)\n",
    "        scheduler.step(f1_macro)\n",
    "        \n",
    "        print(f\"{val_loss=:.4f}; {f1_macro=:.4f}; {f1_macro_best=:.4f}\")\n",
    "\n",
    "        if input() != \"\":\n",
    "            break\n",
    "\n",
    "\n",
    "finally:\n",
    "    torch.save(model.state_dict(), os.path.join(export_path, \"model_last\", \"model.pt\"))\n",
    "\n",
    "    with open(os.path.join(export_path, \"metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diploma",
   "language": "python",
   "name": "diploma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
